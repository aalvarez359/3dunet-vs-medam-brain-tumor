{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOMjslXqQs9jr1dTNng+tvc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aalvarez359/3dunet-vs-medam-brain-tumor/blob/main/metrics/unet_4mod_viz_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdNTwApy9CcE",
        "outputId": "e7c8c9ba-088a-4152-9c7a-bf72ec0e9f16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nibabel segmentation-models-pytorch-3d torch torchvision torchaudio tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcNQ8Vy79F5p",
        "outputId": "0b986de1-0a2f-46eb-a304-54199f612e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.12/dist-packages (5.3.2)\n",
            "Collecting segmentation-models-pytorch-3d\n",
            "  Downloading segmentation_models_pytorch_3d-1.0.2-py3-none-any.whl.metadata (724 bytes)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from nibabel) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from nibabel) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.12/dist-packages (from nibabel) (4.15.0)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch-3d)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch-3d)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting timm==0.9.7 (from segmentation-models-pytorch-3d)\n",
            "  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm-3d==1.0.1 (from segmentation-models-pytorch-3d)\n",
            "  Downloading timm_3d-1.0.1-py3-none-any.whl.metadata (588 bytes)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch-3d) (11.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch-3d) (1.17.0)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch-3d)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm==0.9.7->segmentation-models-pytorch-3d) (6.0.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from timm==0.9.7->segmentation-models-pytorch-3d) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm==0.9.7->segmentation-models-pytorch-3d) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->timm==0.9.7->segmentation-models-pytorch-3d) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->timm==0.9.7->segmentation-models-pytorch-3d) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.9.7->segmentation-models-pytorch-3d) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.9.7->segmentation-models-pytorch-3d) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.9.7->segmentation-models-pytorch-3d) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.9.7->segmentation-models-pytorch-3d) (2025.11.12)\n",
            "Downloading segmentation_models_pytorch_3d-1.0.2-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm_3d-1.0.1-py3-none-any.whl (626 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.8/626.8 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16426 sha256=d1dd11a3122fe14298687f4981079d5217a2d44347e11ff4492674d62855d5c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/3f/43/e6271c7026fe08c185da2be23c98c8e87477d3db63f41f32ad\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=a5c4370b56274408303d04281af1258c4158b2097003b0fb4c1270f442d13c64\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/01/56/40a48f75dbdfe167a0cb70d3b48913369a00ec5c4e9fed5f2b\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, efficientnet-pytorch, timm-3d, timm, pretrainedmodels, segmentation-models-pytorch-3d\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.22\n",
            "    Uninstalling timm-1.0.22:\n",
            "      Successfully uninstalled timm-1.0.22\n",
            "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-3d-1.0.2 timm-0.9.7 timm-3d-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "New"
      ],
      "metadata": {
        "id": "MoQshJF9_Lj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Colab-ready script (no CLI):\n",
        "- Set the CONFIG values below, then just \"Run\" the cell.\n",
        "- Plots training curves from training_history.json in ckpt_dir.\n",
        "- Optionally runs sliding-window inference on one case to save a slice PNG.\n",
        "\"\"\"\n",
        "\n",
        "# ======================\n",
        "# ====== CONFIG ========\n",
        "# ======================\n",
        "CKPT_DIR   = \"/content/drive/MyDrive/BrainTumor_Checkpoints\"  # folder with training_history.json and *.pt\n",
        "OUT_DIR    = \"./plots\"                                        # where to save PNGs\n",
        "\n",
        "# Graphs (always produced)\n",
        "MAKE_GRAPHS = True\n",
        "\n",
        "# Slice viz (set to True only if you want the qualitative slice)\n",
        "MAKE_SLICE  = True  # change to False if you only want the curves\n",
        "\n",
        "# Which checkpoint to use for the slice (exact filename in CKPT_DIR)\n",
        "CKPT_NAME   = \"best_epoch65_dice0.7330.pt\"  # or \"last.pt\"\n",
        "\n",
        "# Paths to one case (only needed if MAKE_SLICE=True)\n",
        "IMAGE_NII   = \"/content/drive/MyDrive/Task01_BrainTumour_extracted/Task01_BrainTumour/imagesTr/BRATS_137.nii.gz\"  # (H,W,D,4) for 4-mod MSD\n",
        "LABEL_NII   = \"/content/drive/MyDrive/Task01_BrainTumour_extracted/Task01_BrainTumour/labelsTr/BRATS_137.nii.gz\"  # (H,W,D), optional\n",
        "\n",
        "# Slice display options\n",
        "SLICE_AXIS  = \"H\"     # \"D\", \"H\", or \"W\"\n",
        "SLICE_INDEX = None     # None = auto-pick slice with max tumor; otherwise explicit index\n",
        "\n",
        "# Sliding window overlap (for inference)\n",
        "OVERLAP     = 0.75\n",
        "\n",
        "# If your training used a different patch, this will be overridden by ckpt.config.patch_size if present.\n",
        "PATCH_SIZE  = (128, 224, 224)\n",
        "\n",
        "# Decoder channels used in your training model (kept slimmer for memory)\n",
        "DECODER_CHANNELS = (192, 128, 64, 32, 16)\n",
        "\n",
        "# ======================\n",
        "# ===== END CONFIG =====\n",
        "# ======================\n",
        "\n",
        "import os, json\n",
        "from typing import Any, Dict, List, Optional\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "try:\n",
        "    import nibabel as nib\n",
        "except Exception:\n",
        "    nib = None\n",
        "\n",
        "try:\n",
        "    import segmentation_models_pytorch_3d as smp3d\n",
        "except Exception:\n",
        "    smp3d = None\n",
        "\n",
        "\n",
        "# ============================\n",
        "# ===== HISTORY UTILITIES ====\n",
        "# ============================\n",
        "\n",
        "def _load_history_json(hist_path: str):\n",
        "    with open(hist_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    if isinstance(data, dict) and \"history\" in data:\n",
        "        return data[\"history\"]\n",
        "    return data\n",
        "\n",
        "def _extract_from_list_of_dicts(hist_list: List[Dict[str, Any]]):\n",
        "    epochs, mean_dice, per_class = [], [], []\n",
        "    for i, e in enumerate(hist_list, start=1):\n",
        "        ep = int(e.get(\"epoch\", i))\n",
        "        md = (\n",
        "            e.get(\"val_meanDice\")\n",
        "            or e.get(\"val_meanDice(no-bg)\")\n",
        "            or e.get(\"val_meanDice_no_bg\")\n",
        "            or e.get(\"meanDice\")\n",
        "            or e.get(\"val_mean_dice\")\n",
        "        )\n",
        "        pc = (\n",
        "            e.get(\"per_class_dice\")\n",
        "            or e.get(\"val_per_class_dice\")\n",
        "            or e.get(\"dice_per_class\")\n",
        "            or e.get(\"val_dice_per_class\")\n",
        "            or e.get(\"per_class\")\n",
        "        )\n",
        "        epochs.append(ep)\n",
        "        mean_dice.append(float(md) if md is not None else np.nan)\n",
        "\n",
        "        # normalize per-class to [bg, edema, non_enh, enh]\n",
        "        if isinstance(pc, dict):\n",
        "            order = [\"bg\", \"edema\", \"non_enh\", \"enh\"]\n",
        "            if all(k in pc for k in order):\n",
        "                per_class.append([float(pc[k]) for k in order])\n",
        "            elif all(str(k) in pc for k in range(4)):\n",
        "                per_class.append([float(pc[str(k)]) for k in range(4)])\n",
        "            else:\n",
        "                try:\n",
        "                    items = sorted(pc.items(), key=lambda x: x[0])\n",
        "                    per_class.append([float(v) for _, v in items[:4]])\n",
        "                except Exception:\n",
        "                    per_class.append([np.nan]*4)\n",
        "        elif isinstance(pc, (list, tuple)) and len(pc) >= 3:\n",
        "            if len(pc) >= 4:\n",
        "                per_class.append([float(pc[0]), float(pc[1]), float(pc[2]), float(pc[3])])\n",
        "            else:\n",
        "                # assume [edema, non_enh, enh]\n",
        "                per_class.append([np.nan, float(pc[0]), float(pc[1]), float(pc[2])])\n",
        "        else:\n",
        "            per_class.append([np.nan]*4)\n",
        "    return np.array(epochs), np.array(mean_dice), np.array(per_class)\n",
        "\n",
        "def _extract_from_dict_of_lists(hist_dict: Dict[str, List[Any]]):\n",
        "    # matches your trainer keys: epoch, val_mean_dice, dice_edema, dice_non_enh, dice_enh\n",
        "    ep    = np.array(hist_dict.get(\"epoch\", []), dtype=int)\n",
        "    md    = np.array(hist_dict.get(\"val_mean_dice\", []), dtype=float)\n",
        "    edema = np.array(hist_dict[\"dice_edema\"],    dtype=float) if \"dice_edema\"    in hist_dict else None\n",
        "    non_enh = np.array(hist_dict[\"dice_non_enh\"], dtype=float) if \"dice_non_enh\" in hist_dict else None\n",
        "    enh   = np.array(hist_dict[\"dice_enh\"],      dtype=float) if \"dice_enh\"      in hist_dict else None\n",
        "\n",
        "    lengths = [\n",
        "        len(ep),\n",
        "        len(md),\n",
        "        len(edema)   if edema   is not None else 0,\n",
        "        len(non_enh) if non_enh is not None else 0,\n",
        "        len(enh)     if enh     is not None else 0,\n",
        "    ]\n",
        "    max_len = max(lengths) if lengths else 0\n",
        "\n",
        "    def pad(a):\n",
        "        if a is None:\n",
        "            return np.full((max_len,), np.nan, dtype=float)\n",
        "        a = np.asarray(a, dtype=float)\n",
        "        if len(a) < max_len:\n",
        "            out = np.full((max_len,), np.nan, dtype=float)\n",
        "            out[:len(a)] = a\n",
        "            return out\n",
        "        return a\n",
        "\n",
        "    ep    = pad(ep)\n",
        "    md    = pad(md)\n",
        "    edema = pad(edema)\n",
        "    non_enh = pad(non_enh)\n",
        "    enh   = pad(enh)\n",
        "\n",
        "    per_class = np.stack([np.full_like(md, np.nan), edema, non_enh, enh], axis=1)\n",
        "    return ep, md, per_class\n",
        "\n",
        "def load_and_extract_history(ckpt_dir: str):\n",
        "    hist_path = os.path.join(ckpt_dir, \"training_history.json\")\n",
        "    if not os.path.exists(hist_path):\n",
        "        raise FileNotFoundError(f\"training_history.json not found in {ckpt_dir}\")\n",
        "    data = _load_history_json(hist_path)\n",
        "    if isinstance(data, list):\n",
        "        return _extract_from_list_of_dicts(data)\n",
        "    elif isinstance(data, dict):\n",
        "        return _extract_from_dict_of_lists(data)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported history JSON format.\")\n",
        "\n",
        "def plot_mean_dice(epochs, mean_dice, out_path):\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, mean_dice, linewidth=2)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Validation Mean Dice (no-bg)\")\n",
        "    plt.title(\"Mean Dice vs Epoch\")\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    plt.savefig(out_path, bbox_inches=\"tight\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "def plot_per_class(epochs, per_class, out_path):\n",
        "    if per_class.ndim != 2 or per_class.shape[1] < 3:\n",
        "        print(\"Per-class dice not found; skipping per-class plot.\")\n",
        "        return\n",
        "    labels = [\"bg\",\"edema\",\"non-enh\",\"enh\"]\n",
        "    plt.figure()\n",
        "    for i in range(min(4, per_class.shape[1])):\n",
        "        plt.plot(epochs, per_class[:, i], linewidth=2, label=labels[i])\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Dice per Class\")\n",
        "    plt.title(\"Per-class Dice vs Epoch\")\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
        "    plt.legend()\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    plt.savefig(out_path, bbox_inches=\"tight\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# ============================\n",
        "# ===== INFERENCE HELPERS ====\n",
        "# ============================\n",
        "\n",
        "def _to_dhw(a: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Ensure volume is (D,H,W). Handles (H,W,D) or (D,H,W).\"\"\"\n",
        "    if a.ndim != 3:\n",
        "        raise ValueError(\"Expected 3D volume\")\n",
        "    if a.shape[-1] < a.shape[0] and a.shape[-1] < a.shape[1]:\n",
        "        a = np.moveaxis(a, -1, 0)  # (D,H,W)\n",
        "    return a\n",
        "\n",
        "@torch.no_grad()\n",
        "def sliding_window_inference(volume_cdhw: np.ndarray,\n",
        "                             model: torch.nn.Module,\n",
        "                             window: tuple,\n",
        "                             overlap: float,\n",
        "                             device: str = \"cuda\"):\n",
        "    \"\"\"\n",
        "    Generic sliding-window inference for (C,D,H,W) volumes.\n",
        "    Uses model.n_classes if present, else 4.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    C, D, H, W = volume_cdhw.shape\n",
        "    wd, wh, ww = window\n",
        "    sd = max(1, int(wd * (1 - overlap)))\n",
        "    sh = max(1, int(wh * (1 - overlap)))\n",
        "    sw = max(1, int(ww * (1 - overlap)))\n",
        "\n",
        "    n_classes = int(getattr(model, \"n_classes\", 4))\n",
        "    out_prob = torch.zeros((n_classes, D, H, W), dtype=torch.float32, device=device)\n",
        "    out_norm = torch.zeros((1, D, H, W), dtype=torch.float32, device=device)\n",
        "\n",
        "    for z in range(0, max(D - wd + 1, 1), sd):\n",
        "        z0 = min(z, D - wd)\n",
        "        for y in range(0, max(H - wh + 1, 1), sh):\n",
        "            y0 = min(y, H - wh)\n",
        "            for x in range(0, max(W - ww + 1, 1), sw):\n",
        "                x0 = min(x, W - ww)\n",
        "                patch = volume_cdhw[:, z0:z0+wd, y0:y0+wh, x0:x0+ww]\n",
        "                pt = torch.from_numpy(patch).unsqueeze(0).to(device)  # 1,C,D,H,W\n",
        "                logits = model(pt)\n",
        "                probs = torch.softmax(logits, dim=1)[0]\n",
        "                out_prob[:, z0:z0+wd, y0:y0+wh, x0:x0+ww] += probs\n",
        "                out_norm[:, z0:z0+wd, y0:y0+wh, x0:x0+ww] += 1.0\n",
        "\n",
        "    out_prob /= (out_norm + 1e-8)\n",
        "    pred = torch.argmax(out_prob, dim=0)  # D H W\n",
        "    return pred.cpu().numpy()\n",
        "\n",
        "def _build_model_from_ckpt(ckpt: Dict[str, Any], device: str = \"cuda\"):\n",
        "    \"\"\"\n",
        "    Rebuild the SMP 3D U-Net from checkpoint config (if present),\n",
        "    otherwise fall back to 4-mod MSD defaults.\n",
        "    \"\"\"\n",
        "    if smp3d is None:\n",
        "        raise SystemExit(\"segmentation-models-pytorch-3d not installed. Run: pip install segmentation-models-pytorch-3d\")\n",
        "\n",
        "    cfg = ckpt.get(\"config\", {}) if isinstance(ckpt, dict) else {}\n",
        "    encoder     = cfg.get(\"encoder\", \"efficientnet-b7\")\n",
        "    in_channels = int(cfg.get(\"in_channels\", 4))\n",
        "    classes     = int(cfg.get(\"classes\", 4))\n",
        "    patch_size  = tuple(cfg.get(\"patch_size\", PATCH_SIZE))\n",
        "\n",
        "    model = smp3d.Unet(\n",
        "        encoder_name=encoder,\n",
        "        encoder_weights=None,\n",
        "        in_channels=in_channels,\n",
        "        classes=classes,\n",
        "        decoder_channels=DECODER_CHANNELS,\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    # Attach helpers\n",
        "    model.n_classes = classes\n",
        "    model.patch_size = patch_size\n",
        "    model.in_channels_manual = in_channels\n",
        "    return model\n",
        "\n",
        "def _load_image_chwd(image_nii_path: str, in_channels: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Load a NIfTI volume and return (C,D,H,W) with per-channel z-score normalization.\n",
        "    Supports:\n",
        "      - 4-mod MSD/BRATS: (H,W,D,4) when in_channels == 4\n",
        "      - single-modality: (H,W,D)  when in_channels == 1\n",
        "    \"\"\"\n",
        "    if nib is None:\n",
        "        raise SystemExit(\"nibabel not installed. Run: pip install nibabel\")\n",
        "\n",
        "    img = nib.load(image_nii_path).get_fdata(dtype=np.float32)\n",
        "\n",
        "    # 4-mod case: (H,W,D,4)\n",
        "    if img.ndim == 4 and img.shape[-1] == in_channels:\n",
        "        chans = []\n",
        "        for c in range(in_channels):\n",
        "            v = img[..., c]\n",
        "            mask = (v != 0)\n",
        "            m = v[mask].mean() if mask.any() else v.mean()\n",
        "            s = v[mask].std()  if mask.any() else v.std()\n",
        "            s = s if s > 1e-8 else 1.0\n",
        "            chans.append(((v - m) / s).astype(np.float32))\n",
        "        vol = np.stack(chans, axis=3)        # H W D C\n",
        "        vol = np.transpose(vol, (3, 2, 0, 1))# C D H W\n",
        "        return vol\n",
        "\n",
        "    # single-modality: (H,W,D)\n",
        "    if img.ndim == 3 and in_channels == 1:\n",
        "        v = img\n",
        "        mask = (v != 0)\n",
        "        m = v[mask].mean() if mask.any() else v.mean()\n",
        "        s = v[mask].std()  if mask.any() else v.std()\n",
        "        s = s if s > 1e-8 else 1.0\n",
        "        v_norm = ((v - m) / s).astype(np.float32)\n",
        "        vol = v_norm[None, ...]              # (1,H,W,D)\n",
        "        vol = np.transpose(vol, (0, 3, 1, 2))# C D H W\n",
        "        return vol\n",
        "\n",
        "    raise ValueError(\n",
        "        f\"Unexpected image shape {img.shape} for in_channels={in_channels}. \"\n",
        "        f\"Expected (H,W,D,{in_channels}) or (H,W,D) if in_channels==1.\"\n",
        "    )\n",
        "\n",
        "def _best_slice_index(mask_dhw: np.ndarray, axis: int) -> int:\n",
        "    \"\"\"\n",
        "    Choose the slice index with the largest number of True voxels along the given axis.\n",
        "    If mask is empty, returns the middle slice.\n",
        "    \"\"\"\n",
        "    if mask_dhw is None or mask_dhw.ndim != 3:\n",
        "        return 0\n",
        "    if axis == 0:   # D\n",
        "        counts = mask_dhw.reshape(mask_dhw.shape[0], -1).sum(axis=1)\n",
        "    elif axis == 1: # H\n",
        "        counts = mask_dhw.transpose(1,0,2).reshape(mask_dhw.shape[1], -1).sum(axis=1)\n",
        "    else:           # W\n",
        "        counts = mask_dhw.transpose(2,0,1).reshape(mask_dhw.shape[2], -1).sum(axis=1)\n",
        "    if counts.max() == 0:\n",
        "        L = mask_dhw.shape[axis]\n",
        "        return L // 2\n",
        "    return int(np.argmax(counts))\n",
        "\n",
        "def _overlay_multiclass(ax, base2d: np.ndarray, label2d: Optional[np.ndarray], title: str):\n",
        "    \"\"\"\n",
        "    Show base2d in grayscale, then overlay tumor classes:\n",
        "      1 (edema) -> yellow, 2 (non-enh) -> green, 3 (enh) -> red.\n",
        "    \"\"\"\n",
        "    img = base2d.astype(np.float32)\n",
        "    if np.isfinite(img).all():\n",
        "        p2, p98 = np.percentile(img, [2, 98])\n",
        "        if p98 > p2:\n",
        "            ax.imshow(img, cmap=\"gray\", vmin=p2, vmax=p98)\n",
        "        else:\n",
        "            ax.imshow(img, cmap=\"gray\")\n",
        "    else:\n",
        "        ax.imshow(img, cmap=\"gray\")\n",
        "\n",
        "    if label2d is not None:\n",
        "        H, W = label2d.shape\n",
        "        class_colors = {\n",
        "            1: (1.0, 1.0, 0.0, 0.40),  # edema: yellow\n",
        "            2: (0.0, 1.0, 0.0, 0.40),  # non-enh: green\n",
        "            3: (1.0, 0.0, 0.0, 0.40),  # enh: red\n",
        "        }\n",
        "        for cls, (r, g, b, a) in class_colors.items():\n",
        "            mask = (label2d == cls)\n",
        "            if np.any(mask):\n",
        "                overlay = np.zeros((H, W, 4), dtype=np.float32)\n",
        "                overlay[..., 0] = r\n",
        "                overlay[..., 1] = g\n",
        "                overlay[..., 2] = b\n",
        "                overlay[..., 3] = a * mask.astype(np.float32)\n",
        "                ax.imshow(overlay, interpolation=\"none\")\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "def visualize_slice_png(pred_dhw: np.ndarray,\n",
        "                        label_dhw: Optional[np.ndarray],\n",
        "                        image_dhw: Optional[np.ndarray],\n",
        "                        slice_axis: str,\n",
        "                        slice_index: Optional[int],\n",
        "                        out_path: str):\n",
        "    \"\"\"\n",
        "    - If slice_index is None, auto-pick slice with most tumor (label if available, else prediction).\n",
        "    - Shows:\n",
        "       [0] image (gray)\n",
        "       [1] label overlay (if provided)\n",
        "       [2] prediction overlay\n",
        "    \"\"\"\n",
        "    axis_map = {\"D\": 0, \"H\": 1, \"W\": 2}\n",
        "    ax = axis_map.get(slice_axis.upper(), 1)\n",
        "\n",
        "    # Choose slice index\n",
        "    if slice_index is None:\n",
        "        if label_dhw is not None:\n",
        "            idx = _best_slice_index((label_dhw > 0), ax)\n",
        "        else:\n",
        "            idx = _best_slice_index((pred_dhw > 0), ax)\n",
        "    else:\n",
        "        idx = int(slice_index)\n",
        "\n",
        "    def slc(a):\n",
        "        if a is None:\n",
        "            return None\n",
        "        if ax == 0:\n",
        "            return a[idx, :, :]\n",
        "        elif ax == 1:\n",
        "            return a[:, idx, :]\n",
        "        else:\n",
        "            return a[:, :, idx]\n",
        "\n",
        "    img2d  = slc(image_dhw)\n",
        "    lab2d  = slc(label_dhw)\n",
        "    pred2d = slc(pred_dhw)\n",
        "\n",
        "    # If no image channel provided, synthesize zeros\n",
        "    if img2d is None:\n",
        "        base2d = np.zeros_like(lab2d if lab2d is not None else pred2d, dtype=np.float32)\n",
        "    else:\n",
        "        base2d = img2d\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "    # Image\n",
        "    axs[0].imshow(base2d, cmap=\"gray\")\n",
        "    axs[0].set_title(\"Image\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    # Label overlay\n",
        "    _overlay_multiclass(axs[1], base2d, lab2d, \"Label (overlay)\")\n",
        "\n",
        "    # Prediction overlay\n",
        "    _overlay_multiclass(axs[2], base2d, pred2d, \"Prediction (overlay)\")\n",
        "\n",
        "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
        "    plt.savefig(out_path, bbox_inches=\"tight\", dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(\"Saved:\", out_path)\n",
        "\n",
        "\n",
        "# ============================\n",
        "# ========== MAIN ============\n",
        "# ============================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "    # 1) Graphs\n",
        "    if MAKE_GRAPHS:\n",
        "        epochs, mean_dice, per_class = load_and_extract_history(CKPT_DIR)\n",
        "        out_mean = os.path.join(OUT_DIR, \"mean_dice_vs_epoch.png\")\n",
        "        out_pcls = os.path.join(OUT_DIR, \"per_class_dice_vs_epoch.png\")\n",
        "        plot_mean_dice(epochs, mean_dice, out_mean)\n",
        "        plot_per_class(epochs, per_class, out_pcls)\n",
        "        print(\"Saved:\", out_mean)\n",
        "        print(\"Saved:\", out_pcls)\n",
        "\n",
        "    # 2) One qualitative slice (optional)\n",
        "    if MAKE_SLICE:\n",
        "        ckpt_path = os.path.join(CKPT_DIR, CKPT_NAME)\n",
        "        if not os.path.exists(ckpt_path):\n",
        "            raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
        "\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # Load checkpoint & rebuild model from its config\n",
        "        ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "        model = _build_model_from_ckpt(ckpt, device=device)\n",
        "\n",
        "        # Load weights\n",
        "        state = ckpt[\"state_dict\"] if isinstance(ckpt, dict) and \"state_dict\" in ckpt else ckpt\n",
        "        model.load_state_dict(state, strict=True)\n",
        "\n",
        "        # Prepare image volume (C,D,H,W)\n",
        "        in_channels = int(getattr(model, \"in_channels_manual\", 4))\n",
        "        vol_cdhw = _load_image_chwd(IMAGE_NII, in_channels=in_channels)\n",
        "\n",
        "        # Patch size: prefer what was saved in config, else global PATCH_SIZE\n",
        "        patch_size = tuple(getattr(model, \"patch_size\", PATCH_SIZE))\n",
        "\n",
        "        pred_dhw = sliding_window_inference(vol_cdhw, model, patch_size, OVERLAP, device=device)\n",
        "\n",
        "        # Optional label and an image channel for context\n",
        "        lab_dhw = None\n",
        "        img_dhw = None\n",
        "\n",
        "        if LABEL_NII and os.path.exists(LABEL_NII) and nib is not None:\n",
        "            lab = nib.load(LABEL_NII).get_fdata().astype(np.int64)  # (H,W,D)\n",
        "            lab_dhw = _to_dhw(lab)\n",
        "\n",
        "        if IMAGE_NII and os.path.exists(IMAGE_NII) and nib is not None:\n",
        "            raw = nib.load(IMAGE_NII).get_fdata()\n",
        "            if raw.ndim == 4:\n",
        "                raw = raw[..., 0]  # show channel 0\n",
        "            img_dhw = _to_dhw(raw)\n",
        "\n",
        "        out_slice = os.path.join(OUT_DIR, \"qualitative_slice.png\")\n",
        "        visualize_slice_png(pred_dhw, lab_dhw, img_dhw, SLICE_AXIS, SLICE_INDEX, out_slice)\n",
        "\n",
        "    print(\"Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTcJCLwR_Fll",
        "outputId": "25909c08-a79c-4195-c070-cd32e3735668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: ./plots/mean_dice_vs_epoch.png\n",
            "Saved: ./plots/per_class_dice_vs_epoch.png\n",
            "Saved: ./plots/qualitative_slice.png\n",
            "Done.\n"
          ]
        }
      ]
    }
  ]
}